<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>CUDA笔记1 | 橙的notebook</title><meta name="author" content="橙"><meta name="copyright" content="橙"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="CUDA 笔记CUDA笔记。 Reference：  https:&#x2F;&#x2F;face2ai.com&#x2F;program-blog&#x2F;#GPU编程（CUDA） https:&#x2F;&#x2F;github.com&#x2F;HeKun-NVIDIA&#x2F;CUDA-Programming-Guide-in-Chinese CUDA by example  主要参考是Professional CUDA C Programming](https:">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA笔记1">
<meta property="og:url" content="https://a-y-1.github.io/2024/10/10/CUDA%20NOTE/index.html">
<meta property="og:site_name" content="橙的notebook">
<meta property="og:description" content="CUDA 笔记CUDA笔记。 Reference：  https:&#x2F;&#x2F;face2ai.com&#x2F;program-blog&#x2F;#GPU编程（CUDA） https:&#x2F;&#x2F;github.com&#x2F;HeKun-NVIDIA&#x2F;CUDA-Programming-Guide-in-Chinese CUDA by example  主要参考是Professional CUDA C Programming](https:">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://a-y-1.github.io/cover/cuda.jpg">
<meta property="article:published_time" content="2024-10-10T04:54:29.000Z">
<meta property="article:modified_time" content="2024-10-10T06:28:36.937Z">
<meta property="article:author" content="橙">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://a-y-1.github.io/cover/cuda.jpg"><link rel="shortcut icon" href="/img/orange.png"><link rel="canonical" href="https://a-y-1.github.io/2024/10/10/CUDA%20NOTE/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CUDA笔记1',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-10-10 14:28:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="APlayer.min.css"><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js" async></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/a0.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">53</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/../cover/cuda.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="橙的notebook"><span class="site-name">橙的notebook</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">CUDA笔记1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-10T04:54:29.000Z" title="发表于 2024-10-10 12:54:29">2024-10-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-10T06:28:36.937Z" title="更新于 2024-10-10 14:28:36">2024-10-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="CUDA笔记1"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="CUDA-笔记"><a href="#CUDA-笔记" class="headerlink" title="CUDA 笔记"></a>CUDA 笔记</h1><p>CUDA笔记。</p>
<p>Reference：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//face2ai.com/program-blog/%23GPU%E7%BC%96%E7%A8%8B%EF%BC%88CUDA%EF%BC%89">https://face2ai.com/program-blog/#GPU编程（CUDA）</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/HeKun-NVIDIA/CUDA-Programming-Guide-in-Chinese">https://github.com/HeKun-NVIDIA/CUDA-Programming-Guide-in-Chinese</a></li>
<li>CUDA by example</li>
</ul>
<p>主要参考是Professional CUDA C Programming](<a target="_blank" rel="noopener" href="https://www.cs.utexas.edu/~rossbach/cs380p/papers/cuda-programming.pdf)这本书和第一个链接博主的笔记。">https://www.cs.utexas.edu/~rossbach/cs380p/papers/cuda-programming.pdf)这本书和第一个链接博主的笔记。</a></p>
<h2 id="1-CUDA-编程模型"><a href="#1-CUDA-编程模型" class="headerlink" title="1 CUDA 编程模型"></a>1 CUDA 编程模型</h2><h3 id="1-1-CUDA并行编程模型概述"><a href="#1-1-CUDA并行编程模型概述" class="headerlink" title="1.1 CUDA并行编程模型概述"></a>1.1 CUDA并行编程模型概述</h3><h4 id="1-1-1-内存管理"><a href="#1-1-1-内存管理" class="headerlink" title="1.1.1 内存管理"></a>1.1.1 内存管理</h4><p>CUDA提供了API分配管理设备上的内存，这些API也可以分配管理主机上的内存。这些API和标准库的内存管理API是对应的。</p>
<p>API: cudaMalloc    cudaMemcpy    cudaMemSet    cudaFree</p>
<p>其中cudaMemcpy实现主机到设备的数据拷贝：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMemcpy</span><span class="params">(<span class="type">void</span> * dst,<span class="type">const</span> <span class="type">void</span> * src,<span class="type">size_t</span> count,</span></span><br><span class="line"><span class="params">  cudaMemcpyKind kind)</span>;</span><br></pre></td></tr></table></figure>
<p>cudaMemcpyKind kind:</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
</ul>
<p>返回的结果有两种， cudaSuccess或cudaErrorMemoryAllocation，可以用以下接口得到详细信息：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">char</span>* <span class="title function_">cudaGetErrorString</span><span class="params">(cudaError_t error)</span>;</span><br></pre></td></tr></table></figure>
<p>CUDA的内存层次可以大概用以下模型描述：</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918111230434.png" alt="image-20240918111230434" width="50%"></p>
<p>为了区分设备和主机内存，在分配时可以给变量添加后缀_d和_h。从而<strong>避免混用设备和主机的内存地址</strong>。</p>
<h4 id="1-1-2-线程管理"><a href="#1-1-2-线程管理" class="headerlink" title="1.1.2 线程管理"></a>1.1.2 线程管理</h4><p>组织GPU的线程是进行并行化时的一个主要问题。在GPU中，一个核函数包含一个grid，一个grid包含多个块，每个块中有多个线程。</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918112113544.png" alt="image-20240918112113544" width="33%"></p>
<p>一个线程块中的线程可以进行同步，并共享内存。线程的编号是由两个结构体确定的：</p>
<ul>
<li>blockIdx：块在网格中的索引。</li>
<li>threadIdx：线程在块内的索引。</li>
</ul>
<p>这两个结构体都包含三个无符号整数字段x，y，z。还有两个结构体保存上述x，y，z字段的范围：</p>
<ul>
<li>blockDim</li>
<li>gridDim</li>
</ul>
<p>这两个结构体同样包含x，y，z三个字段。为dim3类型。</p>
<p>一个网格通常被分为二维的块，每个块常被分为三维的线程。dim3是在主机定义的，不可修改。其各维度的限制后续介绍。</p>
<h4 id="1-1-3-核函数"><a href="#1-1-3-核函数" class="headerlink" title="1.1.3 核函数"></a>1.1.3 核函数</h4><p>核函数是在GPU上运行的代码，这部分代码是NVCC编译的。核函数都通过以下方式启动：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;grid,block&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure>
<p>可以使用dim3类型的grid和block配置内核函数运行，也可以直接使用常量/int变量初始化。例如：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;<span class="number">4</span>,<span class="number">8</span>&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure>
<p>对应的线程布局为：</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918140101986.png" alt="image-20240918140101986" width="70%"></p>
<p>当主机启动核函数，控制流会回到主机代码。可以通过以下API进行同步：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaDeviceSynchronize</span><span class="params">(<span class="type">void</span>)</span>;</span><br></pre></td></tr></table></figure>
<p>以上是显式进行同步。在启动核函数之后，cudaMemcpy等API也会进行隐式的同步。</p>
<p>核函数的声明限定符有三种，分别为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>限定符</th>
<th>执行</th>
<th>调用</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>__global__</td>
<td>设备</td>
<td>主机或计算能力3以上的设备</td>
<td>返回void</td>
</tr>
<tr>
<td>__device__</td>
<td>设备</td>
<td>设备</td>
<td></td>
</tr>
<tr>
<td>__host__</td>
<td>主机</td>
<td>主机</td>
<td>可省略</td>
</tr>
</tbody>
</table>
</div>
<p>核函数有以下限制：</p>
<ul>
<li>只能访问设备内存</li>
<li>必须返回void</li>
<li>不支持可变数量参数</li>
<li>不支持静态变量</li>
<li>显式异步</li>
</ul>
<h4 id="1-1-4-错误处理"><a href="#1-1-4-错误处理" class="headerlink" title="1.1.4 错误处理"></a>1.1.4 错误处理</h4><p>由于CUDA核函数是异步执行的，为了处理和方便发现错误，要检查调用的返回值，例如如下的宏：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(call)\</span></span><br><span class="line"><span class="meta">&#123;\</span></span><br><span class="line"><span class="meta">  const cudaError_t <span class="keyword">error</span>=call;\</span></span><br><span class="line"><span class="meta">  <span class="keyword">if</span>(<span class="keyword">error</span>!=cudaSuccess)\</span></span><br><span class="line"><span class="meta">  &#123;\</span></span><br><span class="line"><span class="meta">      printf(<span class="string">&quot;ERROR: %s:%d,&quot;</span>,__FILE__,__LINE__);\</span></span><br><span class="line"><span class="meta">      printf(<span class="string">&quot;code:%d,reason:%s\n&quot;</span>,<span class="keyword">error</span>,cudaGetErrorString(<span class="keyword">error</span>));\</span></span><br><span class="line"><span class="meta">      exit(1);\</span></span><br><span class="line"><span class="meta">  &#125;\</span></span><br><span class="line"><span class="meta">&#125;</span></span><br></pre></td></tr></table></figure>
<p>在编写代码时，处理错误信息可以方便找到错误。在release版本中可以去掉错误检查。</p>
<h3 id="1-2-核函数计时"><a href="#1-2-核函数计时" class="headerlink" title="1.2 核函数计时"></a>1.2 核函数计时</h3><p>性能优化时常常对函数运行时间进行计时。然而，核函数的启动和返回本身是需要时间的，因此在启动前开始计时，同步后计时结束的时间和执行时间有偏差。</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918143153777.png" alt="image-20240918143153777" width="50%"></p>
<p>和c/c++程序使用perf进行性能分析一样，CUDA程序也可以使用一个名为nvprof的工具进行性能分析。</p>
<p>nvprof需要开启权限，在linux上sudo运行，在win上需要通过nvdia控制面板的开发者模式开启权限。win上还可能提示缺少.dll文件，可以在安装路径/extras下寻找.dll移动到bin下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof [nvprof_args] &lt;application&gt;[application_args]</span><br></pre></td></tr></table></figure>
<p>更详细的profile参数和指导见<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#。">https://docs.nvidia.com/cuda/profiler-users-guide/index.html#。</a></p>
<p><strong>nvprof已经不支持计算能力7.5以上的设备</strong>，只能做基本的计时，现在的设备基本都需要使用nsight system(nsys)进行详细性能分析，需要注意nsys分析需要应用路径合法。</p>
<h3 id="1-3-组织并行线程"><a href="#1-3-组织并行线程" class="headerlink" title="1.3 组织并行线程"></a>1.3 组织并行线程</h3><p>在2.2.2节已经初步介绍了线程的可组织方式。对于二维网格，下图可以直观的体现每个线程的编号：</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918145100584.png" alt="image-20240918145100584" width="50%"></p>
<p>这是用ix iy表示线程，如果用全局ID表示线程编号，为blockDim.x*blockDim.y*gridDim.x*blockIdx.y + blockDim.x*blockDim.y*blockIdx.x+threadIdx.y*blockDim.x+threadIdx.x。</p>
<p>当然，在设备上，数据都是线性存储的，例如一个二维矩阵：</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918145222734.png" alt="image-20240918145222734" width="60%"></p>
<p>线程和数据的映射关系可以自行设计，最简单的方式就是ix，iy的线程处理二维的(ix，iy)的数据。<strong>不同的线程的组织方式能得到不同的性能</strong>，在后续章节会介绍具体的原因。</p>
<h3 id="1-4-设备信息查询"><a href="#1-4-设备信息查询" class="headerlink" title="1.4 设备信息查询"></a>1.4 设备信息查询</h3><p>CUDA C提供了cudaGetDeviceCount获取设备数量，并可以使用cudaGetDeviceProperties( &amp;prop, i )获取设备的具体信息，从而选择合适的设备。可以为prop赋值，然后通过cudaChooseDevice( &amp;dev, &amp;prop )条件筛选合适的设备，cudaSetDevice( dev )决定使用的设备。CUDA by example提供了一份输出主要设备信息的示例代码，包含以下的信息：</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918150638009.png" alt="image-20240918150638009" width="50%"></p>
<h2 id="2-CUDA-执行模型"><a href="#2-CUDA-执行模型" class="headerlink" title="2 CUDA 执行模型"></a>2 CUDA 执行模型</h2><h3 id="2-1-架构概述"><a href="#2-1-架构概述" class="headerlink" title="2.1 架构概述"></a>2.1 架构概述</h3><p>GPU架构是基于流式多处理器(SM)搭建的。GPU中包含多个SM，SM的结构如下图：</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918160427915.png" alt="image-20240918160427915" width="50%"></p>
<p>这张图还省略了一些其他组件，例如寄存器文件和线程束调度器。CUDA使用SIMT并行。调度执行线程束，不同设备有不同线程束大小，通常为32。每个SM可以被分配到多个block，一个block可以有多个线程，但SM每次只执行一个线程束。线程块被分配到SM上时，会被分成多个线程束，线程束在SM上交替执行。</p>
<h3 id="2-2-线程束执行"><a href="#2-2-线程束执行" class="headerlink" title="2.2 线程束执行"></a>2.2 线程束执行</h3><h4 id="2-2-1-线程束分化"><a href="#2-2-1-线程束分化" class="headerlink" title="2.2.1 线程束分化"></a>2.2.1 线程束分化</h4><p>线程束执行时执行的是相同的指令，处理自己的数据。如果一个线程束的不同线程包含不同的控制条件，就会导致线程束的分化。例如一个if else条件选择块，每个线程都会执行所有的if和else部分，一部分线程执行if的部分，而其他线程只能等待，然后所有线程进入else代码块，上一步等待的线程再执行else的部分，其他线程等待。</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918162741874.png" alt="image-20240918162741874" width="80%"></p>
<p>如果线程束所有的线程都执行if或else时，就不会产生上述问题，因此可以控制一个线程束的线程进入同一个分支，例如：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">mathKernel2</span><span class="params">(<span class="type">float</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> tid = blockIdx.x* blockDim.x + threadIdx.x;</span><br><span class="line">	<span class="type">float</span> a = <span class="number">0.0</span>;</span><br><span class="line">	<span class="type">float</span> b = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">if</span> ((tid/warpSize) % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">		a = <span class="number">100.0f</span>;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		b = <span class="number">200.0f</span>;</span><br><span class="line">	c[tid] = a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-2-2-线程束执行的资源分配"><a href="#2-2-2-线程束执行的资源分配" class="headerlink" title="2.2.2 线程束执行的资源分配"></a>2.2.2 线程束执行的资源分配</h4><p>线程束可以是已经执行的，也可以是已经分配到SM，但还未进行资源的分配。</p>
<p>每个SM上有多少个线程束可以处于执行状态，取决于寄存器等资源。kernel占用的资源越少，更多的线程就能同时执行。</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918171255332.png" alt="image-20240918171255332" width="50%"></p>
<h4 id="2-2-3-隐藏延迟"><a href="#2-2-3-隐藏延迟" class="headerlink" title="2.2.3 隐藏延迟"></a>2.2.3 隐藏延迟</h4><p>延迟有两种，计算延迟和访存延迟。必须有足够的线程束用于调度，才能最大效率使用计算资源和带宽。对于计算延迟，所需线程束是延迟 * 吞吐量。延迟是计算指令需要的时钟周期，吞吐量是单周期能进行的操作。</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20240918172727525.png" alt="image-20240918172727525" width="50%"></p>
<p>类似的，访存延迟的隐藏需要访存延迟*每个时钟周期可读取的数据量才能充分利用到GPU带宽。</p>
<p>线程束的下界为计算核心数<em>单条指令延迟、例如一个计算的延迟为32个时钟周期，则最少需要32\</em>20个线程使设备满载。</p>
<h3 id="2-3-并行性表现"><a href="#2-3-并行性表现" class="headerlink" title="2.3 并行性表现"></a>2.3 并行性表现</h3><p>本节通过修改核函数的配置，进行性能分析，进一步理解线程束执行的过程。本节使用的核函数为矩阵加法。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>::chrono;</span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">matrixAdd</span><span class="params">(<span class="type">float</span> *ma, <span class="type">float</span> *mb, <span class="type">float</span> *mc, <span class="type">int</span> nx, <span class="type">int</span> ny)</span> &#123;</span><br><span class="line">  <span class="type">int</span> ix = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line">  <span class="type">int</span> iy = threadIdx.y + blockDim.y * blockIdx.y;</span><br><span class="line">  <span class="type">int</span> idx = ix + iy * ny;</span><br><span class="line">  <span class="keyword">if</span> (ix &lt; nx &amp;&amp; iy &lt; ny) &#123;</span><br><span class="line">    ma[idx] = mb[idx] + mc[idx];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span> &#123;</span><br><span class="line">  <span class="type">int</span> nx = <span class="number">1e4</span>, ny = <span class="number">1e4</span>;</span><br><span class="line">  <span class="type">int</span> nbyte = nx * ny * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">  <span class="type">float</span> *a_h = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nbyte);</span><br><span class="line">  <span class="type">float</span> *b_h = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nbyte);</span><br><span class="line">  <span class="type">float</span> *c_h = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nbyte);</span><br><span class="line">  <span class="type">float</span> *a_d, *b_d, *c_d;</span><br><span class="line">  cudaMalloc(&amp;a_d, nbyte);</span><br><span class="line">  cudaMalloc(&amp;b_d, nbyte);</span><br><span class="line">  cudaMalloc(&amp;c_d, nbyte);</span><br><span class="line">  cudaMemcpy(a_d, a_h, nbyte, cudaMemcpyHostToDevice);</span><br><span class="line">  cudaMemcpy(b_d, b_h, nbyte, cudaMemcpyHostToDevice);</span><br><span class="line">  <span class="type">int</span> dimx = argc &gt; <span class="number">2</span> ? atoi(argv[<span class="number">1</span>]) : <span class="number">32</span>;</span><br><span class="line">  <span class="type">int</span> dimy = argc &gt; <span class="number">2</span> ? atoi(argv[<span class="number">2</span>]) : <span class="number">32</span>;</span><br><span class="line">  dim3 <span class="title function_">block</span><span class="params">(dimx, dimy)</span>;</span><br><span class="line">  dim3 <span class="title function_">grid</span><span class="params">((nx - <span class="number">1</span>) / block.x + <span class="number">1</span>, (ny - <span class="number">1</span>) / block.y + <span class="number">1</span>)</span>;</span><br><span class="line">  <span class="keyword">auto</span> start = high_resolution_clock::now();</span><br><span class="line">  matrixAdd&lt;&lt;&lt;grid, block&gt;&gt;&gt;(a_d, b_d, c_d, nx, ny);</span><br><span class="line">  <span class="keyword">auto</span> end = high_resolution_clock::now();</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;time:%lf\n&quot;</span>, duration_cast&lt;duration&lt;<span class="type">double</span>&gt;&gt;(end - start).count());</span><br><span class="line">  cudaDeviceSynchronize();</span><br><span class="line">  cudaMemcpy(c_h, c_d, nbyte, cudaMemcpyDeviceToHost);</span><br><span class="line">  cudaFree(a_d);</span><br><span class="line">  cudaFree(b_d);</span><br><span class="line">  cudaFree(c_d);</span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(c_h);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用nsys做profile，自己的gpu为gtx1650，metrics-set为tu11x，可以通过<code>--gpu-metrics-set=help</code>看支持的metrics-set。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsys profile --gpu-metrics-set tu11x --gpu-metrics-devices 0 --stats=true ./main</span><br></pre></td></tr></table></figure>
<p>profile后较为细节的数据都在sqlite里面，可视化界面打开nsys-rep报告可以直观看到数据。但是nsys得到的是整体宏观的数据，不便观察更详细的核函数运行数据，因此用另一个工具nsight compute再做profile，记录不同线程块配置的一些数据：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>block-size</th>
<th>Achieved Occupancy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>32x32</td>
<td>87.34</td>
<td>0.000054</td>
</tr>
<tr>
<td>16x32</td>
<td>82.68</td>
<td>0.000084</td>
</tr>
<tr>
<td>16x16</td>
<td>85.65</td>
<td>0.000077</td>
</tr>
<tr>
<td>32x16</td>
<td>82.68</td>
<td>0.000083</td>
</tr>
</tbody>
</table>
</div>
<p>可以看到规整的threadBlock有更好的效果，对于当前这个小计算任务，32x32的较大threadBlock的SM Occupancy是最高的，这也符合前面隐藏延迟的理论，因为这个计算并不复杂，占用的资源并不多，尽可能多的线程可以在SM上充分利用硬件资源完成计算。</p>
<p>除了achieved occupancy，nsight compute还能测量许多其他的指标，在调优时可能都需要关注。例如访存相关的数据：</p>
<p><img src="/2024/10/10/CUDA%20NOTE/image-20241009163421168.png" alt="image-20241009163421168" width="67%"></p>
<p>说明这个程序的线程组织方式完全没有利用到L1 Cache，仍有很大的改进空间。</p>
<h3 id="2-4-避免分支分化"><a href="#2-4-避免分支分化" class="headerlink" title="2.4 避免分支分化"></a>2.4 避免分支分化</h3><p>本节从并行归约问题出发，学习GPU并行编程时避免分支分化的方法。</p>
<p>归约是常见的并行化的步骤，用于对分块的结果进行处理。首先考虑最简单的规约，对向量求元素和。并行时各个线程计算一组元素，得到的结果归约迭代求和。常见的元素划分方式有两种：相邻元素配对和交错配对。</p>
<p><img src="/2024/10/10/CUDA%20NOTE/xianglin.png" alt="img" width="50%"></p>
<p><img src="/2024/10/10/CUDA%20NOTE/jiaocuo.png" alt="img" width="50%"></p>
<p>以下是CPU的交错配对归约：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">recursiveReduce</span><span class="params">(<span class="type">int</span> *data, <span class="type">int</span> <span class="type">const</span> size)</span></span>&#123;</span><br><span class="line">	<span class="comment">// terminate check</span></span><br><span class="line">	<span class="keyword">if</span> (size == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> data[<span class="number">0</span>];</span><br><span class="line">	<span class="comment">// renew the stride</span></span><br><span class="line">	<span class="type">int</span> <span class="type">const</span> stride = size / <span class="number">2</span>;</span><br><span class="line">	<span class="keyword">if</span> (size % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; stride; i++)</span><br><span class="line">			data[i] += data[i + stride];</span><br><span class="line">		data[<span class="number">0</span>] += data[size - <span class="number">1</span>];</span><br><span class="line">    &#125;<span class="keyword">else</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; stride; i++)</span><br><span class="line">			data[i] += data[i + stride];</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">recursiveReduce</span>(data, stride);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在GPU上，首先编写相邻元素配对的核函数，每个线程块计算一组数据，最后归约所有线程块的结果。这里假定了数据的总数是2的幂次方，如果不是需要考虑边界问题。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">reductSum</span><span class="params">(<span class="type">int</span> *in, <span class="type">int</span> *out, <span class="type">int</span> size)</span> &#123;</span><br><span class="line">  <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> *a = in + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span> (blockIdx.x * blockDim.x + tid &gt;= size)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tid % (<span class="number">2</span> * stride) == <span class="number">0</span>) &#123;</span><br><span class="line">      a[tid] = a[tid] + a[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    out[blockIdx.x] = a[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/2024/10/10/CUDA%20NOTE/3_22.png" alt="img" width="50%"></p>
<p>线程从一开始就只有1/2会进行计算，其他都在进行分支判断后等待，每一个线程束都只有一半的线程计算，却会使用整个SM。如果能将计算的线程控制在一起，不计算的线程控制在一起，不计算的线程的分支都不执行，SM就不会调度运行包含这些线程的线程束，节省了资源，从而提高了效率。</p>
<p><img src="/2024/10/10/CUDA%20NOTE/3_23.png" alt="img" width="50%"></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">reductSumOpt</span><span class="params">(<span class="type">int</span> *in, <span class="type">int</span> *out, <span class="type">int</span> size)</span>&#123;</span><br><span class="line">  <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> *a = in + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span>(blockIdx.x * blockDim.x + tid &gt;= size) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>)&#123;</span><br><span class="line">    <span class="type">int</span> idx = <span class="number">2</span> * stride * tid;</span><br><span class="line">    <span class="keyword">if</span>(idx &lt; blockDim.x)&#123;</span><br><span class="line">      a[idx] += a[idx + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    out[blockIdx.x] = a[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Correct. basic ver T=0.006428</span><br><span class="line">Correct. opt ver T=0.003778</span><br></pre></td></tr></table></figure>
<p>实现了1.7x的提升。这是非常显著的，和参考博客作者的性能提升也类似，说明实际的硬件执行情况是符合预期的。接下来对采集数据进行分析，nsight compute没有inst_per_warp这个指标了，但是指令相关的数据也还是可以观察到明显的不同：</p>
<img src="/2024/10/10/CUDA%20NOTE/image-20241009201545033.png" class title="image-20241009201545033">
<center>未优化的kernel</center>

<img src="/2024/10/10/CUDA%20NOTE/image-20241009201645470.png" class title="image-20241009201645470">
<center>优化后的kernel</center>

<p>调整线程分支处理后，执行的指令数下降了，未参与计算的线程束的分支指令没有调度执行。另外，访存的效率也提升了，MemoryThroughput从23.74提升到了44.56。</p>
<p>除了以上的方式，改用交错配对的方式也能将计算的线程控制在一起，如下图所示：</p>
<p><img src="/2024/10/10/CUDA%20NOTE/3_24.png" alt="img" width="50%"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reductSumInterleaved</span><span class="params">(<span class="type">int</span> *in, <span class="type">int</span> *out, <span class="type">int</span> size)</span></span>&#123;</span><br><span class="line">  <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> *a = in + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span>(blockIdx.x * blockDim.x + tid &gt;= size) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> stride = blockDim.x/<span class="number">2</span>; stride&gt;<span class="number">0</span>; stride/=<span class="number">2</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(tid &lt; stride)&#123;</span><br><span class="line">      a[tid] += a[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">    out[blockIdx.x] = a[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Correct. basic ver T=0.006622</span><br><span class="line">Correct. opt ver T=0.003767</span><br><span class="line">Correct. interleaved ver T=0.003731</span><br></pre></td></tr></table></figure>
<p>执行效率与上面的优化方式是一样的。从分支的角度考虑，和上面的优化一样，让靠后的线程束内的线程全部不进入分支，避免被调度执行从而提高了效率。同样的，从profile数据来看是否执行情况是符合预期的。</p>
<p>指令的执行情况如下，比上面的方式要略好一些，但是具体的原因还不太明确。</p>
<img src="/2024/10/10/CUDA%20NOTE/image-20241009211736838.png" class title="image-20241009211736838">
<p>从访存的角度来看，MemoryThroughput降低到了35，L1Cache和L2Cache的缓存命中率都下降了，这是符合预期的，因为起初的访存跨度是最大的，此后逐渐减小，而前两种方式的访存跨度是一开始较小，后面更大，后面跨度大时，数据已经载入到缓存了。因为截至这一节尚未学习内存模型，因此也可能和设想的这个原因不同。此外，虽然缓存命中率明显下降，但却没有影响性能，这是因为写回的数据变少了，上面两种方式写回L2cache的数据是这种的四倍，因为写入的数据是交错的，导致不需要写入的数据也进行了写入。这也充分说明了最佳的性能要通过访存和指令执行(运算)等多方面权衡得到。</p>
<h3 id="2-5-循环展开"><a href="#2-5-循环展开" class="headerlink" title="2.5 循环展开"></a>2.5 循环展开</h3><p>CPU执行代码中的循环展开很多时候编译器都可以完成，GPU执行的代码，，编译器是否能进行循环展开优化，考虑到编译器也在不断更新，还是需要实际测试一下。</p>
<p>上一节中的交错归约每个线程只处理了对应部分的数据，为了提高线程块的负载，对其进行循环展开，考虑用一个线程块处理多块数据。在一开始的时候就将相邻块的数据添加到线程对应位置上。</p>
<img src="/2024/10/10/CUDA%20NOTE/1.png" class title="img">
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">reductSumInterleavedUnroll</span><span class="params">(<span class="type">int</span> *in, <span class="type">int</span> *out, <span class="type">int</span> size)</span>&#123;</span><br><span class="line">  <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> *a = in + blockIdx.x * blockDim.x * <span class="number">2</span>;</span><br><span class="line">  <span class="type">int</span> idx = blockIdx.x * blockDim.x * <span class="number">2</span> + tid;</span><br><span class="line">  <span class="keyword">if</span>(idx &gt;= size) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">if</span>(idx + blockDim.x &lt; size) a[tid] += a[tid + blockDim.x];</span><br><span class="line">  __syncthreads();</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> stride = blockDim.x/<span class="number">2</span>; stride&gt;<span class="number">0</span>; stride/=<span class="number">2</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(tid &lt; stride)&#123;</span><br><span class="line">      a[tid] += a[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">    out[blockIdx.x] = a[<span class="number">0</span>];  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行时间如下，有显著的性能提升：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Correct. basic ver T=0.006359</span><br><span class="line">Correct. opt ver T=0.003661</span><br><span class="line">Correct. interleaved ver T=0.003305</span><br><span class="line">Correct. interleaved unrolling ver T=0.001069</span><br></pre></td></tr></table></figure>
<p>除了上面这种直接先将块的数据加到当前数据位置的方式，我还尝试了直接对两个块的数据进行交错归约，时间和上述方式基本一致。如果改变线程块的计算块数量，性能还会有进一步提升：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>块数量</th>
<th>执行时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>0.002182</td>
</tr>
<tr>
<td>4</td>
<td>0.001600</td>
</tr>
<tr>
<td>8</td>
<td>0.000957</td>
</tr>
</tbody>
</table>
</div>
<p>到了这里已经实现了相对于最基础的版本4.8x的加速比。用nsight compute观察一下各项指标的变化：</p>
<img src="/2024/10/10/CUDA%20NOTE/image-20241010101831330.png" class title="image-20241010101831330">
<img src="/2024/10/10/CUDA%20NOTE/image-20241010102020002.png" class title="image-20241010102020002">
<p>最直观的是指令数减少了，只有之前版本的16%，因为线程束减少了，每个线程都承担了原来8个线程的计算，但只增加了if分支内的8条语句，MemoryThroughput翻了一倍，因为每个线程都需要8个数据块的数据，这也导致了缓存命中率的下降，但并不影响性能的提升，最后，从L2cache写回DeviceMemory的数据也变成了原来的1/8，降低了写入开销。还有一些其他指标下降了，SM Throughput和SM Busy%等，这再次说明了好的性能是多个方面的均衡，要找到真正的性能瓶颈，对瓶颈优化可以牺牲其他方面的效率。</p>
<p>接下来考虑能不能将最后的64个数的计算进行展开。在交错归约的过程中，每计算一轮，有效线程数和数据都会减半。当线程数小于32时，就会出现最后一个线程束内有线程不进入(tid<stride)分支的情况，因此考虑将最后64个数的计算进行展开，更改的kernel部分如下： <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> stride = blockDim.x/<span class="number">2</span>; stride&gt;<span class="number">32</span>; stride/=<span class="number">2</span>)&#123;</span><br><span class="line">  <span class="keyword">if</span>(tid &lt; stride)&#123;</span><br><span class="line">    a[tid] += a[tid + stride];</span><br><span class="line">  &#125;</span><br><span class="line">  __syncthreads();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(tid&lt;<span class="number">32</span>)&#123;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="type">int</span> *vmem = a;</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">32</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">16</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">8</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">4</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">2</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></stride)分支的情况，因此考虑将最后64个数的计算进行展开，更改的kernel部分如下：></p>
<p>第一次计算是32个线程加上64个数据的后半部分，第二次计算时是前16个线程加上前32个数据的后16个，后16个线程也参与了计算，但计算的数据是无意义的，虽然这后16个线程会修改自己的数据，但是SM上线程的执行是同步的，当这16个线程要修改自己的值时，前16个线程已经读取了需要的后16个线程的上一轮数据，并计算完写入了，这也是要用volatile写入和读取数据的原因。</p>
<p>在实际的测试中，这个优化并没有效果，和上面的8次展开的计算时间是基本完全一致的，说明上述的问题对性能影响不大，但展开的思路仍然是值得学习的。</p>
<p>这里要指出，参考博客的作者通过本步优化得到了可见的性能提升，经过我自己的测试，发现性能的提升并不来源于这里，而很可能是来自于以下代码：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(idx + blockDim.x*(blockPerthread<span class="number">-1</span>) &lt; size)&#123;</span><br><span class="line">  <span class="type">int</span> tmp0;</span><br><span class="line">  tmp0 = a[tid + blockDim.x];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">2</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">3</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">4</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">5</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">6</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">7</span>];</span><br><span class="line">  a[tid] += tmp0 ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果每个语句都是a[tid]的+=操作，时间刚好是作者测试的8次展开的0.0012左右的时间，修改为以上后，时间变为了0.0010xx甚至0.00095，优化了对a[tid]的写入。这里对cpu编程这样的代码编译器会不会自动优化没有印象，但至少GPU是不会的，因此可以得到可见的性能提升。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://a-y-1.github.io">橙</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://a-y-1.github.io/2024/10/10/CUDA%20NOTE/">https://a-y-1.github.io/2024/10/10/CUDA%20NOTE/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://a-y-1.github.io" target="_blank">橙的notebook</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/../cover/cuda.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/10/10/CUDA%20NOTE1/" title="CUDA笔记1"><img class="cover" src="/../cover/cuda.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CUDA笔记1</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/a0.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">橙</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">53</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/A-Y-1"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CUDA-%E7%AC%94%E8%AE%B0"><span class="toc-text">CUDA 笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-CUDA-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">1 CUDA 编程模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-CUDA%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0"><span class="toc-text">1.1 CUDA并行编程模型概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">1.1.1 内存管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2-%E7%BA%BF%E7%A8%8B%E7%AE%A1%E7%90%86"><span class="toc-text">1.1.2 线程管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-3-%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-text">1.1.3 核函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-4-%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86"><span class="toc-text">1.1.4 错误处理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%A0%B8%E5%87%BD%E6%95%B0%E8%AE%A1%E6%97%B6"><span class="toc-text">1.2 核函数计时</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B"><span class="toc-text">1.3 组织并行线程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E8%AE%BE%E5%A4%87%E4%BF%A1%E6%81%AF%E6%9F%A5%E8%AF%A2"><span class="toc-text">1.4 设备信息查询</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-CUDA-%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="toc-text">2 CUDA 执行模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="toc-text">2.1 架构概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C"><span class="toc-text">2.2 线程束执行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E5%88%86%E5%8C%96"><span class="toc-text">2.2.1 线程束分化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D"><span class="toc-text">2.2.2 线程束执行的资源分配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-%E9%9A%90%E8%97%8F%E5%BB%B6%E8%BF%9F"><span class="toc-text">2.2.3 隐藏延迟</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%B9%B6%E8%A1%8C%E6%80%A7%E8%A1%A8%E7%8E%B0"><span class="toc-text">2.3 并行性表现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96"><span class="toc-text">2.4 避免分支分化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E5%BE%AA%E7%8E%AF%E5%B1%95%E5%BC%80"><span class="toc-text">2.5 循环展开</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/CUDA%20NOTE/" title="CUDA笔记1">CUDA笔记1</a><time datetime="2024-10-10T04:54:29.000Z" title="发表于 2024-10-10 12:54:29">2024-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/CUDA%20NOTE1/" title="CUDA笔记1">CUDA笔记1</a><time datetime="2024-10-10T04:54:29.000Z" title="发表于 2024-10-10 12:54:29">2024-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/23/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E4%B8%8E%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" title="《并行计算与高性能计算》简记">《并行计算与高性能计算》简记</a><time datetime="2024-09-23T04:54:29.000Z" title="发表于 2024-09-23 12:54:29">2024-09-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/05/Graph500/" title="Graph500">Graph500</a><time datetime="2024-08-05T14:33:29.000Z" title="发表于 2024-08-05 22:33:29">2024-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/23/MPI4py/" title="MPI4py">MPI4py</a><time datetime="2024-03-23T08:47:30.000Z" title="发表于 2024-03-23 16:47:30">2024-03-23</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/../cover/cuda.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 橙</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>