<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>CUDA笔记1 | 橙的notebook</title><meta name="author" content="橙"><meta name="copyright" content="橙"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="CUDA 笔记CUDA笔记。 Reference：  https:&#x2F;&#x2F;face2ai.com&#x2F;program-blog&#x2F;#GPU编程（CUDA） https:&#x2F;&#x2F;github.com&#x2F;HeKun-NVIDIA&#x2F;CUDA-Programming-Guide-in-Chinese CUDA by example  主要参考是Professional CUDA C Programming这本书和第一个链">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA笔记1">
<meta property="og:url" content="https://a-y-1.github.io/2024/10/10/CUDA%20NOTE1/index.html">
<meta property="og:site_name" content="橙的notebook">
<meta property="og:description" content="CUDA 笔记CUDA笔记。 Reference：  https:&#x2F;&#x2F;face2ai.com&#x2F;program-blog&#x2F;#GPU编程（CUDA） https:&#x2F;&#x2F;github.com&#x2F;HeKun-NVIDIA&#x2F;CUDA-Programming-Guide-in-Chinese CUDA by example  主要参考是Professional CUDA C Programming这本书和第一个链">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://a-y-1.github.io/cover/cuda.jpg">
<meta property="article:published_time" content="2024-10-10T04:54:29.000Z">
<meta property="article:modified_time" content="2024-10-21T02:35:32.473Z">
<meta property="article:author" content="橙">
<meta property="article:tag" content="高性能计算">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://a-y-1.github.io/cover/cuda.jpg"><link rel="shortcut icon" href="/img/orange.png"><link rel="canonical" href="https://a-y-1.github.io/2024/10/10/CUDA%20NOTE1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CUDA笔记1',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2024-10-21 10:35:32'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="APlayer.min.css"><div id="aplayer"></div><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js" async></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/a0.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/../cover/cuda.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="橙的notebook"><span class="site-name">橙的notebook</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">CUDA笔记1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-10T04:54:29.000Z" title="发表于 2024-10-10 12:54:29">2024-10-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-21T02:35:32.473Z" title="更新于 2024-10-21 10:35:32">2024-10-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HPCP/">HPCP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="CUDA笔记1"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="CUDA-笔记"><a href="#CUDA-笔记" class="headerlink" title="CUDA 笔记"></a>CUDA 笔记</h1><p>CUDA笔记。</p>
<p>Reference：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//face2ai.com/program-blog/%23GPU%E7%BC%96%E7%A8%8B%EF%BC%88CUDA%EF%BC%89">https://face2ai.com/program-blog/#GPU编程（CUDA）</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/HeKun-NVIDIA/CUDA-Programming-Guide-in-Chinese">https://github.com/HeKun-NVIDIA/CUDA-Programming-Guide-in-Chinese</a></li>
<li>CUDA by example</li>
</ul>
<p>主要参考是<a target="_blank" rel="noopener" href="https://www.cs.utexas.edu/~rossbach/cs380p/papers/cuda-programming.pdf">Professional CUDA C Programming</a>这本书和第一个链接博主的笔记。</p>
<h2 id="1-CUDA-编程模型"><a href="#1-CUDA-编程模型" class="headerlink" title="1 CUDA 编程模型"></a>1 CUDA 编程模型</h2><h3 id="1-1-CUDA并行编程模型概述"><a href="#1-1-CUDA并行编程模型概述" class="headerlink" title="1.1 CUDA并行编程模型概述"></a>1.1 CUDA并行编程模型概述</h3><h4 id="1-1-1-内存管理"><a href="#1-1-1-内存管理" class="headerlink" title="1.1.1 内存管理"></a>1.1.1 内存管理</h4><p>CUDA提供了API分配管理设备上的内存，这些API也可以分配管理主机上的内存。这些API和标准库的内存管理API是对应的。</p>
<p>API: cudaMalloc    cudaMemcpy    cudaMemSet    cudaFree</p>
<p>其中cudaMemcpy实现主机到设备的数据拷贝：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMemcpy</span><span class="params">(<span class="type">void</span> * dst,<span class="type">const</span> <span class="type">void</span> * src,<span class="type">size_t</span> count,</span></span><br><span class="line"><span class="params">  cudaMemcpyKind kind)</span>;</span><br></pre></td></tr></table></figure>
<p>cudaMemcpyKind kind:</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
</ul>
<p>返回的结果有两种， cudaSuccess或cudaErrorMemoryAllocation，可以用以下接口得到详细信息：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">char</span>* <span class="title function_">cudaGetErrorString</span><span class="params">(cudaError_t error)</span>;</span><br></pre></td></tr></table></figure>
<p>CUDA的内存层次可以大概用以下模型描述：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918111230434.png" alt="image-20240918111230434" width="50%"></p>
<p>为了区分设备和主机内存，在分配时可以给变量添加后缀_d和_h。从而<strong>避免混用设备和主机的内存地址</strong>。</p>
<h4 id="1-1-2-线程管理"><a href="#1-1-2-线程管理" class="headerlink" title="1.1.2 线程管理"></a>1.1.2 线程管理</h4><p>组织GPU的线程是进行并行化时的一个主要问题。在GPU中，一个核函数包含一个grid，一个grid包含多个块，每个块中有多个线程。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918112113544.png" alt="image-20240918112113544" width="33%"></p>
<p>一个线程块中的线程可以进行同步，并共享内存。线程的编号是由两个结构体确定的：</p>
<ul>
<li>blockIdx：块在网格中的索引。</li>
<li>threadIdx：线程在块内的索引。</li>
</ul>
<p>这两个结构体都包含三个无符号整数字段x，y，z。还有两个结构体保存上述x，y，z字段的范围：</p>
<ul>
<li>blockDim</li>
<li>gridDim</li>
</ul>
<p>这两个结构体同样包含x，y，z三个字段。为dim3类型。</p>
<p>一个网格通常被分为二维的块，每个块常被分为三维的线程。dim3是在主机定义的，不可修改。其各维度的限制后续介绍。</p>
<h4 id="1-1-3-核函数"><a href="#1-1-3-核函数" class="headerlink" title="1.1.3 核函数"></a>1.1.3 核函数</h4><p>核函数是在GPU上运行的代码，这部分代码是NVCC编译的。核函数都通过以下方式启动：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;grid,block&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure>
<p>可以使用dim3类型的grid和block配置内核函数运行，也可以直接使用常量/int变量初始化。例如：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;<span class="number">4</span>,<span class="number">8</span>&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure>
<p>对应的线程布局为：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918140101986.png" alt="image-20240918140101986" width="70%"></p>
<p>当主机启动核函数，控制流会回到主机代码。可以通过以下API进行同步：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaDeviceSynchronize</span><span class="params">(<span class="type">void</span>)</span>;</span><br></pre></td></tr></table></figure>
<p>以上是显式进行同步。在启动核函数之后，cudaMemcpy等API也会进行隐式的同步。</p>
<p>核函数的声明限定符有三种，分别为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>限定符</th>
<th>执行</th>
<th>调用</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>__global__</td>
<td>设备</td>
<td>主机或计算能力3以上的设备</td>
<td>返回void</td>
</tr>
<tr>
<td>__device__</td>
<td>设备</td>
<td>设备</td>
<td></td>
</tr>
<tr>
<td>__host__</td>
<td>主机</td>
<td>主机</td>
<td>可省略</td>
</tr>
</tbody>
</table>
</div>
<p>核函数有以下限制：</p>
<ul>
<li>只能访问设备内存</li>
<li>必须返回void</li>
<li>不支持可变数量参数</li>
<li>不支持静态变量</li>
<li>显式异步</li>
</ul>
<h4 id="1-1-4-错误处理"><a href="#1-1-4-错误处理" class="headerlink" title="1.1.4 错误处理"></a>1.1.4 错误处理</h4><p>由于CUDA核函数是异步执行的，为了处理和方便发现错误，要检查调用的返回值，例如如下的宏：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(call)\</span></span><br><span class="line"><span class="meta">&#123;\</span></span><br><span class="line"><span class="meta">  const cudaError_t <span class="keyword">error</span>=call;\</span></span><br><span class="line"><span class="meta">  <span class="keyword">if</span>(<span class="keyword">error</span>!=cudaSuccess)\</span></span><br><span class="line"><span class="meta">  &#123;\</span></span><br><span class="line"><span class="meta">      printf(<span class="string">&quot;ERROR: %s:%d,&quot;</span>,__FILE__,__LINE__);\</span></span><br><span class="line"><span class="meta">      printf(<span class="string">&quot;code:%d,reason:%s\n&quot;</span>,<span class="keyword">error</span>,cudaGetErrorString(<span class="keyword">error</span>));\</span></span><br><span class="line"><span class="meta">      exit(1);\</span></span><br><span class="line"><span class="meta">  &#125;\</span></span><br><span class="line"><span class="meta">&#125;</span></span><br></pre></td></tr></table></figure>
<p>在编写代码时，处理错误信息可以方便找到错误。在release版本中可以去掉错误检查。</p>
<h3 id="1-2-核函数计时"><a href="#1-2-核函数计时" class="headerlink" title="1.2 核函数计时"></a>1.2 核函数计时</h3><p>性能优化时常常对函数运行时间进行计时。然而，核函数的启动和返回本身是需要时间的，因此在启动前开始计时，同步后计时结束的时间和执行时间有偏差。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918143153777.png" alt="image-20240918143153777" width="50%"></p>
<p>和c/c++程序使用perf进行性能分析一样，CUDA程序也可以使用一个名为nvprof的工具进行性能分析。</p>
<p>nvprof需要开启权限，在linux上sudo运行，在win上需要通过nvdia控制面板的开发者模式开启权限。win上还可能提示缺少.dll文件，可以在安装路径/extras下寻找.dll移动到bin下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof [nvprof_args] &lt;application&gt;[application_args]</span><br></pre></td></tr></table></figure>
<p>更详细的profile参数和指导见<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#。">https://docs.nvidia.com/cuda/profiler-users-guide/index.html#。</a></p>
<p><strong>nvprof已经不支持计算能力7.5以上的设备</strong>，只能做基本的计时，现在的设备基本都需要使用nsight system(nsys)进行详细性能分析，需要注意nsys分析需要应用路径合法。</p>
<h3 id="1-3-组织并行线程"><a href="#1-3-组织并行线程" class="headerlink" title="1.3 组织并行线程"></a>1.3 组织并行线程</h3><p>在2.2.2节已经初步介绍了线程的可组织方式。对于二维网格，下图可以直观的体现每个线程的编号：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918145100584.png" alt="image-20240918145100584" width="50%"></p>
<p>这是用ix iy表示线程，如果用全局ID表示线程编号，为blockDim.x*blockDim.y*gridDim.x*blockIdx.y + blockDim.x*blockDim.y*blockIdx.x+threadIdx.y*blockDim.x+threadIdx.x。</p>
<p>当然，在设备上，数据都是线性存储的，例如一个二维矩阵：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918145222734.png" alt="image-20240918145222734" width="60%"></p>
<p>线程和数据的映射关系可以自行设计，最简单的方式就是ix，iy的线程处理二维的(ix，iy)的数据。<strong>不同的线程的组织方式能得到不同的性能</strong>，在后续章节会介绍具体的原因。</p>
<h3 id="1-4-设备信息查询"><a href="#1-4-设备信息查询" class="headerlink" title="1.4 设备信息查询"></a>1.4 设备信息查询</h3><p>CUDA C提供了cudaGetDeviceCount获取设备数量，并可以使用cudaGetDeviceProperties( &amp;prop, i )获取设备的具体信息，从而选择合适的设备。可以为prop赋值，然后通过cudaChooseDevice( &amp;dev, &amp;prop )条件筛选合适的设备，cudaSetDevice( dev )决定使用的设备。CUDA by example提供了一份输出主要设备信息的示例代码，包含以下的信息：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918150638009.png" alt="image-20240918150638009" width="50%"></p>
<h2 id="2-CUDA-执行模型"><a href="#2-CUDA-执行模型" class="headerlink" title="2 CUDA 执行模型"></a>2 CUDA 执行模型</h2><h3 id="2-1-架构概述"><a href="#2-1-架构概述" class="headerlink" title="2.1 架构概述"></a>2.1 架构概述</h3><p>GPU架构是基于流式多处理器(SM)搭建的。GPU中包含多个SM，SM的结构如下图：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918160427915.png" alt="image-20240918160427915" width="50%"></p>
<p>这张图还省略了一些其他组件，例如寄存器文件和线程束调度器。CUDA使用SIMT并行。调度执行线程束，不同设备有不同线程束大小，通常为32。每个SM可以被分配到多个block，一个block可以有多个线程，但SM每次只执行一个线程束。线程块被分配到SM上时，会被分成多个线程束，线程束在SM上交替执行。</p>
<h3 id="2-2-线程束执行"><a href="#2-2-线程束执行" class="headerlink" title="2.2 线程束执行"></a>2.2 线程束执行</h3><h4 id="2-2-1-线程束分化"><a href="#2-2-1-线程束分化" class="headerlink" title="2.2.1 线程束分化"></a>2.2.1 线程束分化</h4><p>线程束执行时执行的是相同的指令，处理自己的数据。如果一个线程束的不同线程包含不同的控制条件，就会导致线程束的分化。例如一个if else条件选择块，每个线程都会执行所有的if和else部分，一部分线程执行if的部分，而其他线程只能等待，然后所有线程进入else代码块，上一步等待的线程再执行else的部分，其他线程等待。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918162741874.png" alt="image-20240918162741874" width="80%"></p>
<p>如果线程束所有的线程都执行if或else时，就不会产生上述问题，因此可以控制一个线程束的线程进入同一个分支，例如：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">mathKernel2</span><span class="params">(<span class="type">float</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> tid = blockIdx.x* blockDim.x + threadIdx.x;</span><br><span class="line">	<span class="type">float</span> a = <span class="number">0.0</span>;</span><br><span class="line">	<span class="type">float</span> b = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">if</span> ((tid/warpSize) % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">		a = <span class="number">100.0f</span>;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		b = <span class="number">200.0f</span>;</span><br><span class="line">	c[tid] = a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-2-2-线程束执行的资源分配"><a href="#2-2-2-线程束执行的资源分配" class="headerlink" title="2.2.2 线程束执行的资源分配"></a>2.2.2 线程束执行的资源分配</h4><p>线程束可以是已经执行的，也可以是已经分配到SM，但还未进行资源的分配。</p>
<p>每个SM上有多少个线程束可以处于执行状态，取决于寄存器等资源。kernel占用的资源越少，更多的线程就能同时执行。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918171255332.png" alt="image-20240918171255332" width="50%"></p>
<h4 id="2-2-3-隐藏延迟"><a href="#2-2-3-隐藏延迟" class="headerlink" title="2.2.3 隐藏延迟"></a>2.2.3 隐藏延迟</h4><p>延迟有两种，计算延迟和访存延迟。必须有足够的线程束用于调度，才能最大效率使用计算资源和带宽。对于计算延迟，所需线程束是延迟 * 吞吐量。延迟是计算指令需要的时钟周期，吞吐量是单周期能进行的操作。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20240918172727525.png" alt="image-20240918172727525" width="50%"></p>
<p>类似的，访存延迟的隐藏需要访存延迟*每个时钟周期可读取的数据量才能充分利用到GPU带宽。</p>
<p>线程束的下界为计算核心数<em>单条指令延迟、例如一个计算的延迟为32个时钟周期，则最少需要32\</em>20个线程使设备满载。</p>
<h3 id="2-3-并行性表现"><a href="#2-3-并行性表现" class="headerlink" title="2.3 并行性表现"></a>2.3 并行性表现</h3><p>本节通过修改核函数的配置，进行性能分析，进一步理解线程束执行的过程。本节使用的核函数为矩阵加法。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>::chrono;</span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">matrixAdd</span><span class="params">(<span class="type">float</span> *ma, <span class="type">float</span> *mb, <span class="type">float</span> *mc, <span class="type">int</span> nx, <span class="type">int</span> ny)</span> &#123;</span><br><span class="line">  <span class="type">int</span> ix = threadIdx.x + blockDim.x * blockIdx.x;</span><br><span class="line">  <span class="type">int</span> iy = threadIdx.y + blockDim.y * blockIdx.y;</span><br><span class="line">  <span class="type">int</span> idx = ix + iy * ny;</span><br><span class="line">  <span class="keyword">if</span> (ix &lt; nx &amp;&amp; iy &lt; ny) &#123;</span><br><span class="line">    ma[idx] = mb[idx] + mc[idx];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span> &#123;</span><br><span class="line">  <span class="type">int</span> nx = <span class="number">1e4</span>, ny = <span class="number">1e4</span>;</span><br><span class="line">  <span class="type">int</span> nbyte = nx * ny * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">  <span class="type">float</span> *a_h = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nbyte);</span><br><span class="line">  <span class="type">float</span> *b_h = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nbyte);</span><br><span class="line">  <span class="type">float</span> *c_h = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nbyte);</span><br><span class="line">  <span class="type">float</span> *a_d, *b_d, *c_d;</span><br><span class="line">  cudaMalloc(&amp;a_d, nbyte);</span><br><span class="line">  cudaMalloc(&amp;b_d, nbyte);</span><br><span class="line">  cudaMalloc(&amp;c_d, nbyte);</span><br><span class="line">  cudaMemcpy(a_d, a_h, nbyte, cudaMemcpyHostToDevice);</span><br><span class="line">  cudaMemcpy(b_d, b_h, nbyte, cudaMemcpyHostToDevice);</span><br><span class="line">  <span class="type">int</span> dimx = argc &gt; <span class="number">2</span> ? atoi(argv[<span class="number">1</span>]) : <span class="number">32</span>;</span><br><span class="line">  <span class="type">int</span> dimy = argc &gt; <span class="number">2</span> ? atoi(argv[<span class="number">2</span>]) : <span class="number">32</span>;</span><br><span class="line">  dim3 <span class="title function_">block</span><span class="params">(dimx, dimy)</span>;</span><br><span class="line">  dim3 <span class="title function_">grid</span><span class="params">((nx - <span class="number">1</span>) / block.x + <span class="number">1</span>, (ny - <span class="number">1</span>) / block.y + <span class="number">1</span>)</span>;</span><br><span class="line">  <span class="keyword">auto</span> start = high_resolution_clock::now();</span><br><span class="line">  matrixAdd&lt;&lt;&lt;grid, block&gt;&gt;&gt;(a_d, b_d, c_d, nx, ny);</span><br><span class="line">  <span class="keyword">auto</span> end = high_resolution_clock::now();</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;time:%lf\n&quot;</span>, duration_cast&lt;duration&lt;<span class="type">double</span>&gt;&gt;(end - start).count());</span><br><span class="line">  cudaDeviceSynchronize();</span><br><span class="line">  cudaMemcpy(c_h, c_d, nbyte, cudaMemcpyDeviceToHost);</span><br><span class="line">  cudaFree(a_d);</span><br><span class="line">  cudaFree(b_d);</span><br><span class="line">  cudaFree(c_d);</span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(c_h);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用nsys做profile，自己的gpu为gtx1650，metrics-set为tu11x，可以通过<code>--gpu-metrics-set=help</code>看支持的metrics-set。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsys profile --gpu-metrics-set tu11x --gpu-metrics-devices 0 --stats=true ./main</span><br></pre></td></tr></table></figure>
<p>profile后较为细节的数据都在sqlite里面，可视化界面打开nsys-rep报告可以直观看到数据。但是nsys得到的是整体宏观的数据，不便观察更详细的核函数运行数据，因此用另一个工具nsight compute再做profile，记录不同线程块配置的一些数据：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>block-size</th>
<th>Achieved Occupancy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>32x32</td>
<td>87.34</td>
<td>0.000054</td>
</tr>
<tr>
<td>16x32</td>
<td>82.68</td>
<td>0.000084</td>
</tr>
<tr>
<td>16x16</td>
<td>85.65</td>
<td>0.000077</td>
</tr>
<tr>
<td>32x16</td>
<td>82.68</td>
<td>0.000083</td>
</tr>
</tbody>
</table>
</div>
<p>可以看到规整的threadBlock有更好的效果，对于当前这个小计算任务，32x32的较大threadBlock的SM Occupancy是最高的，这也符合前面隐藏延迟的理论，因为这个计算并不复杂，占用的资源并不多，尽可能多的线程可以在SM上充分利用硬件资源完成计算。</p>
<p>除了achieved occupancy，nsight compute还能测量许多其他的指标，在调优时可能都需要关注。例如访存相关的数据：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20241009163421168.png" alt="image-20241009163421168" width="67%"></p>
<p>说明这个程序的线程组织方式完全没有利用到L1 Cache，仍有很大的改进空间。</p>
<h3 id="2-4-避免分支分化"><a href="#2-4-避免分支分化" class="headerlink" title="2.4 避免分支分化"></a>2.4 避免分支分化</h3><p>本节从并行归约问题出发，学习GPU并行编程时避免分支分化的方法。</p>
<p>归约是常见的并行化的步骤，用于对分块的结果进行处理。首先考虑最简单的规约，对向量求元素和。并行时各个线程计算一组元素，得到的结果归约迭代求和。常见的元素划分方式有两种：相邻元素配对和交错配对。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/xianglin.png" alt="img" width="50%"></p>
<p><img src="/2024/10/10/CUDA%20NOTE1/jiaocuo.png" alt="img" width="50%"></p>
<p>以下是CPU的交错配对归约：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">recursiveReduce</span><span class="params">(<span class="type">int</span> *data, <span class="type">int</span> <span class="type">const</span> size)</span></span>&#123;</span><br><span class="line">	<span class="comment">// terminate check</span></span><br><span class="line">	<span class="keyword">if</span> (size == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> data[<span class="number">0</span>];</span><br><span class="line">	<span class="comment">// renew the stride</span></span><br><span class="line">	<span class="type">int</span> <span class="type">const</span> stride = size / <span class="number">2</span>;</span><br><span class="line">	<span class="keyword">if</span> (size % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; stride; i++)</span><br><span class="line">			data[i] += data[i + stride];</span><br><span class="line">		data[<span class="number">0</span>] += data[size - <span class="number">1</span>];</span><br><span class="line">    &#125;<span class="keyword">else</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; stride; i++)</span><br><span class="line">			data[i] += data[i + stride];</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">recursiveReduce</span>(data, stride);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在GPU上，首先编写相邻元素配对的核函数，每个线程块计算一组数据，最后归约所有线程块的结果。这里假定了数据的总数是2的幂次方，如果不是需要考虑边界问题。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">reductSum</span><span class="params">(<span class="type">int</span> *in, <span class="type">int</span> *out, <span class="type">int</span> size)</span> &#123;</span><br><span class="line">  <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> *a = in + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span> (blockIdx.x * blockDim.x + tid &gt;= size)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tid % (<span class="number">2</span> * stride) == <span class="number">0</span>) &#123;</span><br><span class="line">      a[tid] = a[tid] + a[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    out[blockIdx.x] = a[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/2024/10/10/CUDA%20NOTE1/3_22.png" alt="img" width="50%"></p>
<p>线程从一开始就只有1/2会进行计算，其他都在进行分支判断后等待，每一个线程束都只有一半的线程计算，却会使用整个SM。如果能将计算的线程控制在一起，不计算的线程控制在一起，不计算的线程的分支都不执行，SM就不会调度运行包含这些线程的线程束，节省了资源，从而提高了效率。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/3_23.png" alt="img" width="50%"></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">reductSumOpt</span><span class="params">(<span class="type">int</span> *in, <span class="type">int</span> *out, <span class="type">int</span> size)</span>&#123;</span><br><span class="line">  <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> *a = in + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span>(blockIdx.x * blockDim.x + tid &gt;= size) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>)&#123;</span><br><span class="line">    <span class="type">int</span> idx = <span class="number">2</span> * stride * tid;</span><br><span class="line">    <span class="keyword">if</span>(idx &lt; blockDim.x)&#123;</span><br><span class="line">      a[idx] += a[idx + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">    out[blockIdx.x] = a[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Correct. basic ver T=0.006428</span><br><span class="line">Correct. opt ver T=0.003778</span><br></pre></td></tr></table></figure>
<p>实现了1.7x的提升。这是非常显著的，和参考博客作者的性能提升也类似，说明实际的硬件执行情况是符合预期的。接下来对采集数据进行分析，nsight compute没有inst_per_warp这个指标了，但是指令相关的数据也还是可以观察到明显的不同：</p>
<img src="/2024/10/10/CUDA%20NOTE1/image-20241009201545033.png" class title="image-20241009201545033">
<center>未优化的kernel</center>

<img src="/2024/10/10/CUDA%20NOTE1/image-20241009201645470.png" class title="image-20241009201645470">
<center>优化后的kernel</center>

<p>调整线程分支处理后，执行的指令数下降了，未参与计算的线程束的分支指令没有调度执行。另外，访存的效率也提升了，MemoryThroughput从23.74提升到了44.56。</p>
<p>除了以上的方式，改用交错配对的方式也能将计算的线程控制在一起，如下图所示：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/3_24.png" alt="img" width="50%"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reductSumInterleaved</span><span class="params">(<span class="type">int</span> *in, <span class="type">int</span> *out, <span class="type">int</span> size)</span></span>&#123;</span><br><span class="line">  <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> *a = in + blockIdx.x * blockDim.x;</span><br><span class="line">  <span class="keyword">if</span>(blockIdx.x * blockDim.x + tid &gt;= size) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> stride = blockDim.x/<span class="number">2</span>; stride&gt;<span class="number">0</span>; stride/=<span class="number">2</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(tid &lt; stride)&#123;</span><br><span class="line">      a[tid] += a[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">    out[blockIdx.x] = a[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Correct. basic ver T=0.006622</span><br><span class="line">Correct. opt ver T=0.003767</span><br><span class="line">Correct. interleaved ver T=0.003731</span><br></pre></td></tr></table></figure>
<p>执行效率与上面的优化方式是一样的。从分支的角度考虑，和上面的优化一样，让靠后的线程束内的线程全部不进入分支，避免被调度执行从而提高了效率。同样的，从profile数据来看是否执行情况是符合预期的。</p>
<p>指令的执行情况如下，比上面的方式要略好一些，但是具体的原因还不太明确。</p>
<img src="/2024/10/10/CUDA%20NOTE1/image-20241009211736838.png" class title="image-20241009211736838">
<p>从访存的角度来看，MemoryThroughput降低到了35，L1Cache和L2Cache的缓存命中率都下降了，这是符合预期的，因为起初的访存跨度是最大的，此后逐渐减小，而前两种方式的访存跨度是一开始较小，后面更大，后面跨度大时，数据已经载入到缓存了。因为截至这一节尚未学习内存模型，因此也可能和设想的这个原因不同。此外，虽然缓存命中率明显下降，但却没有影响性能，这是因为写回的数据变少了，上面两种方式写回L2cache的数据是这种的四倍，因为写入的数据是交错的，导致不需要写入的数据也进行了写入。这也充分说明了最佳的性能要通过访存和指令执行(运算)等多方面权衡得到。</p>
<h3 id="2-5-循环展开"><a href="#2-5-循环展开" class="headerlink" title="2.5 循环展开"></a>2.5 循环展开</h3><p>CPU执行代码中的循环展开很多时候编译器都可以完成，GPU执行的代码，，编译器是否能进行循环展开优化，考虑到编译器也在不断更新，还是需要实际测试一下。</p>
<p>上一节中的交错归约每个线程只处理了对应部分的数据，为了提高线程块的负载，对其进行循环展开，考虑用一个线程块处理多块数据。在一开始的时候就将相邻块的数据添加到线程对应位置上。</p>
<img src="/2024/10/10/CUDA%20NOTE1/1.png" class title="img">
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">reductSumInterleavedUnroll</span><span class="params">(<span class="type">int</span> *in, <span class="type">int</span> *out, <span class="type">int</span> size)</span>&#123;</span><br><span class="line">  <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> *a = in + blockIdx.x * blockDim.x * <span class="number">2</span>;</span><br><span class="line">  <span class="type">int</span> idx = blockIdx.x * blockDim.x * <span class="number">2</span> + tid;</span><br><span class="line">  <span class="keyword">if</span>(idx &gt;= size) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">if</span>(idx + blockDim.x &lt; size) a[tid] += a[tid + blockDim.x];</span><br><span class="line">  __syncthreads();</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> stride = blockDim.x/<span class="number">2</span>; stride&gt;<span class="number">0</span>; stride/=<span class="number">2</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span>(tid &lt; stride)&#123;</span><br><span class="line">      a[tid] += a[tid + stride];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">    out[blockIdx.x] = a[<span class="number">0</span>];  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行时间如下，有显著的性能提升：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Correct. basic ver T=0.006359</span><br><span class="line">Correct. opt ver T=0.003661</span><br><span class="line">Correct. interleaved ver T=0.003305</span><br><span class="line">Correct. interleaved unrolling ver T=0.001069</span><br></pre></td></tr></table></figure>
<p>除了上面这种直接先将块的数据加到当前数据位置的方式，我还尝试了直接对两个块的数据进行交错归约，时间和上述方式基本一致。如果改变线程块的计算块数量，性能还会有进一步提升：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>块数量</th>
<th>执行时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>0.002182</td>
</tr>
<tr>
<td>4</td>
<td>0.001600</td>
</tr>
<tr>
<td>8</td>
<td>0.000957</td>
</tr>
</tbody>
</table>
</div>
<p>到了这里已经实现了相对于最基础的版本4.8x的加速比。用nsight compute观察一下各项指标的变化：</p>
<img src="/2024/10/10/CUDA%20NOTE1/image-20241010101831330.png" class title="image-20241010101831330">
<img src="/2024/10/10/CUDA%20NOTE1/image-20241010102020002.png" class title="image-20241010102020002">
<p>最直观的是指令数减少了，只有之前版本的16%，因为线程束减少了，每个线程都承担了原来8个线程的计算，但只增加了if分支内的8条语句，MemoryThroughput翻了一倍，因为每个线程都需要8个数据块的数据，这也导致了缓存命中率的下降，但并不影响性能的提升，最后，从L2cache写回DeviceMemory的数据也变成了原来的1/8，降低了写入开销。还有一些其他指标下降了，SM Throughput和SM Busy%等，这再次说明了好的性能是多个方面的均衡，要找到真正的性能瓶颈，对瓶颈优化可以牺牲其他方面的效率。</p>
<p>接下来考虑能不能将最后的64个数的计算进行展开。在交错归约的过程中，每计算一轮，有效线程数和数据都会减半。当线程数小于32时，就会出现最后一个线程束内有线程不进入(tid<stride)分支的情况，因此考虑将最后64个数的计算进行展开，更改的kernel部分如下： <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> stride = blockDim.x/<span class="number">2</span>; stride&gt;<span class="number">32</span>; stride/=<span class="number">2</span>)&#123;</span><br><span class="line">  <span class="keyword">if</span>(tid &lt; stride)&#123;</span><br><span class="line">    a[tid] += a[tid + stride];</span><br><span class="line">  &#125;</span><br><span class="line">  __syncthreads();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(tid&lt;<span class="number">32</span>)&#123;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="type">int</span> *vmem = a;</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">32</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">16</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">8</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">4</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">2</span>];</span><br><span class="line">  vmem[tid] += vmem[tid+<span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></stride)分支的情况，因此考虑将最后64个数的计算进行展开，更改的kernel部分如下：></p>
<p>第一次计算是32个线程加上64个数据的后半部分，第二次计算时是前16个线程加上前32个数据的后16个，后16个线程也参与了计算，但计算的数据是无意义的，虽然这后16个线程会修改自己的数据，但是SM上线程的执行是同步的，当这16个线程要修改自己的值时，前16个线程已经读取了需要的后16个线程的上一轮数据，并计算完写入了，这也是要用volatile写入和读取数据的原因。</p>
<p>在实际的测试中，这个优化并没有效果，和上面的8次展开的计算时间是基本完全一致的，说明上述的问题对性能影响不大，但展开的思路仍然是值得学习的。</p>
<p>这里要指出，参考博客的作者通过本步优化得到了可见的性能提升，经过我自己的测试，发现性能的提升并不来源于这里，而很可能是来自于以下代码：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(idx + blockDim.x*(blockPerthread<span class="number">-1</span>) &lt; size)&#123;</span><br><span class="line">  <span class="type">int</span> tmp0;</span><br><span class="line">  tmp0 = a[tid + blockDim.x];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">2</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">3</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">4</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">5</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">6</span>];</span><br><span class="line">  tmp0 += a[tid + blockDim.x * <span class="number">7</span>];</span><br><span class="line">  a[tid] += tmp0 ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果每个语句都是a[tid]的+=操作，时间刚好是作者测试的8次展开的0.0012左右的时间，修改为以上后，时间变为了0.0010xx甚至0.00095，优化了对a[tid]的写入。这里对cpu编程这样的代码编译器会不会自动优化没有印象，但至少GPU是不会的，因此可以得到可见的性能提升。</p>
<h3 id="2-6-动态并行"><a href="#2-6-动态并行" class="headerlink" title="2.6 动态并行"></a>2.6 动态并行</h3><p>CUDA支持在内核中启动内核，能让复杂内核更加有层次，这样就可以动态调整负载，并且通过内核中启动内核减少数据传输消耗及启动开销。在频繁启动内核的场景下，这是有必要的。有些类似openmp的task并行。</p>
<p>内核中启动的子线程网格和线程全部结束后，父线程和网格才可以结束。线程启动子网格是隐式同步的，即所有线程会同步启动子网格和子线程。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/3_26.png" alt="img" style="zoom:50%;"></p>
<p>核函数内部以相同的方式启动新的子网格，完成嵌套启动，书中给出了嵌套启动helloworld和嵌套规约的例子。</p>
<h2 id="3-全局内存"><a href="#3-全局内存" class="headerlink" title="3 全局内存"></a>3 全局内存</h2><h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><p>CUDA的内存模型比CPU要更丰富一些，GPU上的内存设备有：</p>
<ul>
<li>寄存器</li>
<li>共享内存</li>
<li>本地内存</li>
<li>常量内存</li>
<li>纹理内存</li>
<li>全局内存</li>
</ul>
<p>每种内存都有自己的作用域，生命周期和缓存行为。每个线程都有自己的私有的本地内存，而共享内存是线程块内所有线程可见的，所有线程都能读取常量内存和纹理内存，但这两部分是只读的。全局内存，常量内存和纹理内存都有相同的生命周期。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/1-5.png" alt="img" style="zoom: 33%;"></p>
<p><strong>寄存器</strong></p>
<p>寄存器是速度最快的内存。GPU的寄存器比CPU要更多。核函数内声明的变量都存储在寄存器中，定义的常数长度的数组也是在寄存器中分配地址的，而不像CPU在主存上。</p>
<p>寄存器是线程私有的。生命周期是核函数内部。Fermi架构中每个线程最多63个寄存器，Kepler结构最多255个寄存器。如果线程使用的寄存器越少，常驻线程块就越多。SM上并发的线程块越多。</p>
<p>如果变量太多导致寄存器溢出，数据就会存储在本地内存。可以在编译选项中加入-maxrregcount=32控制一个编译单元里所有核函数使用的寄存器最大数量。</p>
<p><strong>本地内存</strong></p>
<p>本地内存的存储的变量只可能是使用未知索引引用的本地数组，可能占用大量寄存器空间的数组或结构体，溢出的变量。现在的设备，本地内存都存储在每个SM的一级缓存或设备的二级缓存上。</p>
<p><strong>共享内存</strong></p>
<p>核函数中__share__修饰的内存称为共享内存。每个SM都有一定数量的由线程块分配的共享内存，共享内存是片上内存，延迟低，带宽高。其类似于一级缓存，但是可以被编程。共享内存和寄存器一样，需要避免使用过多导致活跃线程束数量减少。</p>
<p>共享内存在核函数内声明，生命周期和线程块一致。因为共享内存是块内线程可见的，所以就有竞争问题的存在，需要通过同步避免内存竞争。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure>
<p>注意，__syncthreads();频繁使用会影响内核执行效率。</p>
<p>SM中的一级缓存，和共享内存共享一个64k的片上内存，他们通过静态划分，划分彼此的容量，运行时可以通过下面语句进行设置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaFuncSetCacheConfig(const void * func,enum cudaFuncCache);</span><br></pre></td></tr></table></figure>
<p>这个函数可以设置内核的共享内存和一级缓存之间的比例。cudaFuncCache参数可选如下配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaFuncCachePreferNone//无参考值，默认设置</span><br><span class="line">cudaFuncCachePreferShared//48k共享内存，16k一级缓存</span><br><span class="line">cudaFuncCachePreferL1// 48k一级缓存，16k共享内存</span><br><span class="line">cudaFuncCachePreferEqual// 32k一级缓存，32k共享内存</span><br></pre></td></tr></table></figure>
<p>Fermi架构支持前三种，后面的设备都支持。</p>
<p><strong>常量内存</strong></p>
<p>常量内存驻留在设备内存中，每个SM都有专用的常量内存缓存。内存常量通过__constant__在核函数外声明。对于所有设备，只可以声明64k的常量内存，常量内存静态声明，并对同一编译单元中的所有核函数可见。常量内存被主机端初始化后不能被核函数修改，初始化函数如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMemcpyToSymbol</span><span class="params">(<span class="type">const</span> <span class="type">void</span>* symbol,<span class="type">const</span> <span class="type">void</span> *src,<span class="type">size_t</span> count)</span>;</span><br></pre></td></tr></table></figure>
<p>当线程束中所有线程都从相同的地址取数据时，常量内存表现较好，比如执行某一个多项式计算，系数都存在常量内存里效率会非常高，但是如果不同的线程取不同地址的数据，常量内存就不那么好了，因为常量内存的读取机制是：<br>一次读取会广播给所有线程束内的线程。</p>
<p><strong>纹理内存</strong></p>
<p>纹理内存驻留在设备内存中，在每个SM的只读缓存中缓存，纹理内存是通过指定的缓存访问的全局内存，只读缓存包括硬件滤波的支持，它可以将浮点插入作为读取过程中的一部分来执行，纹理内存是对二维空间局部性的优化。<br>总的来说纹理内存设计目的应该是为了GPU本职工作显示设计的，但是对于某些特定的程序可能效果更好，比如需要滤波的程序，可以直接通过硬件完成。</p>
<p><strong>全局内存</strong></p>
<p>一般在主机端代码里定义，也可以在设备端定义，不过需要加修饰符，只要不销毁，是和应用程序同生命周期的。使用__device__在设备代码中静态声明全局内存变量。</p>
<p>因为全局内存的性质，当有多个核函数同时执行的时候，如果使用到了同一全局变量，应注意内存竞争。</p>
<p><strong>缓存</strong></p>
<p>与CPU缓存类似，GPU缓存不可编程，其行为出厂是时已经设定好了。GPU上有4种缓存：</p>
<ol>
<li>一级缓存</li>
<li>二级缓存</li>
<li>只读常量缓存</li>
<li>只读纹理缓存</li>
</ol>
<p>每个SM都有一个一级缓存，所有SM共用一个二级缓存。一级二级缓存的作用都是被用来存储本地内存和全局内存中的数据，也包括寄存器溢出的部分。Fermi，Kepler以及以后的设备，CUDA允许我们配置读操作的数据是使用一级缓存和二级缓存，还是只使用二级缓存。<br>与CPU不同的是，CPU读写过程都有可能被缓存，但是GPU写的过程不被缓存，只有加载会被缓存。<br>每个SM有一个只读常量缓存，只读纹理缓存，它们用于设备内存中提高来自于各自内存空间内的读取性能。</p>
<p><strong>静态全局变量</strong></p>
<p>除了cudaMalloc的动态分配全局内存，还可以在全局内存上静态分配变量，调用的是常量的memCpy接口：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line">__device__ <span class="type">float</span> devData;</span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">checkGlobalVariable</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Device: The value of the global variable is %f\n&quot;</span>,devData);</span><br><span class="line">    devData+=<span class="number">2.0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> value=<span class="number">3.14f</span>;</span><br><span class="line">    cudaMemcpyToSymbol(devData,&amp;value,<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Host: copy %f to the global variable\n&quot;</span>,value);</span><br><span class="line">    checkGlobalVariable&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">    cudaMemcpyFromSymbol(&amp;value,devData,<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Host: the value changed by the kernel to %f \n&quot;</span>,value);</span><br><span class="line">    cudaDeviceReset();</span><br><span class="line">    <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意与常量不同的是参数是变量标识符，而不是取址。并且这个内存也不能通过cudaMemcpy赋值。</p>
<h3 id="3-2-内存管理"><a href="#3-2-内存管理" class="headerlink" title="3.2 内存管理"></a>3.2 内存管理</h3><h4 id="3-2-1内存的分配，释放与数据传输"><a href="#3-2-1内存的分配，释放与数据传输" class="headerlink" title="3.2.1内存的分配，释放与数据传输"></a>3.2.1内存的分配，释放与数据传输</h4><p>在全局内存分配和释放方面，GPU和CPUS是类似的，使用cudaMalloc分配数据，并通过传输获取主机的数据。CPU到GPU的内存传输非常慢，因此要尽可能减少传输。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/4-3.png" alt="4-3" style="zoom: 33%;"></p>
<p>主机内存是分页管理的，一块内存可能在不连续的页上，可能发生物理换页，为了避免传输操作中出现换页，CUDA驱动会锁定页面，或分配固定的主机内存，再进行两次数据移动。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/4-4.png" alt="4-4" style="zoom: 33%;"></p>
<p>左侧就是两次数据移动。如果要使用右侧得到预先分配固定内存，需要使用以下函数：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMallocHost</span><span class="params">(<span class="type">void</span> ** devPtr,<span class="type">size_t</span> count)</span></span><br><span class="line">cudaError_t <span class="title function_">cudaFreeHost</span><span class="params">(<span class="type">void</span> *ptr)</span></span><br></pre></td></tr></table></figure>
<p>这些内存的数据可以直接传输到设备，因此memcpy的时间比主机普通malloc内存进行传输的时间更短。但是这样的固定内存的释放和分配成本比可分页内存高，而且会影响实际的可用物理内存空间。</p>
<h4 id="3-2-2零拷贝内存"><a href="#3-2-2零拷贝内存" class="headerlink" title="3.2.2零拷贝内存"></a>3.2.2零拷贝内存</h4><p>一般情况，主机不能访问设备内存，设备不能直接访问主机内存，但零拷贝内存是个例外。GPU可以直接访问零拷贝内存，这部分内存在主机内存。</p>
<p>零拷贝内存是固定内存，不可分页，并且使用时要注意与主机的内存竞争。使用以下方式分配：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaHostAlloc</span><span class="params">(<span class="type">void</span> ** pHost,<span class="type">size_t</span> count,<span class="type">unsigned</span> <span class="type">int</span> flags)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>cudaHostAllocDefalt</li>
<li>cudaHostAllocPortable</li>
<li>cudaHostAllocWriteCombined</li>
</ul>
<ul>
<li>cudaHostAllocMapped</li>
</ul>
<p>cudaHostAllocDefalt和cudaMallocHost函数一致，cudaHostAllocPortable函数返回能被所有CUDA上下文使用的固定内存，cudaHostAllocWriteCombined返回写结合内存，在某些设备上这种内存传输效率更高。cudaHostAllocMapped产生零拷贝内存。<br>零拷贝内存虽然不需要显式的传递到设备上，但是设备还不能通过pHost直接访问对应的内存地址，设备需要访问主机上的零拷贝内存，需要先获得另一个地址，这个地址帮助设备访问到主机对应的内存，方法是：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaHostGetDevicePointer</span><span class="params">(<span class="type">void</span> ** pDevice,<span class="type">void</span> * pHost,<span class="type">unsigned</span> flags)</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>FLAGS必须为0,。零拷贝内存比设备主存更慢，在CPU和GPU共用内存等特殊情况下才有较好的表现。</p>
<h4 id="3-2-3统一虚拟寻址"><a href="#3-2-3统一虚拟寻址" class="headerlink" title="3.2.3统一虚拟寻址"></a>3.2.3统一虚拟寻址</h4><p>在设备架构2.0后，NVIDA提出了UVA，设备内存和主机内存就都映射到相同的虚拟内存地址空间，使用上面的零拷贝内存，就不需要获取DEVICE POINTER了。</p>
<h4 id="3-2-4统一内存寻址"><a href="#3-2-4统一内存寻址" class="headerlink" title="3.2.4统一内存寻址"></a>3.2.4统一内存寻址</h4><p>设备在6.0版本后，NVIDIA又提出了统一内存寻址。统一内存中创建一个托管内存池，内存池中的空间可被主机和设备用相同指针访问。底层系统会自动进行设备和主机间的数据传输，是完全透明的。 在后续小节会介绍统一内存寻址。</p>
<h3 id="3-3-内存访问模式"><a href="#3-3-内存访问模式" class="headerlink" title="3.3 内存访问模式"></a>3.3 内存访问模式</h3><p>许多应用都是访存受限型，最大程度的利用全局内存的高带宽是非常重要的。前述的执行模型已经说明了cuda执行的基本单位是线程束，本节要研究的是线程束对全局内存的访问情况。</p>
<p>全局内存在硬件上大致是下图所示。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/1-1.png" alt="1-1" style="zoom:50%;"></p>
<p>SM独占L1，共享L2，还有常量缓存和只读缓存。</p>
<p>核函数从全局内存读取数据只有两种粒度：32字节 / 128字节。(有点类似缓存行)</p>
<p>如果启用一级缓存，加载数据的粒度为128字节，如果通过编译指令停用L1只用L2，粒度是32字节。</p>
<p>SM执行的基础是线程束，只要有一个线程访存，其他31个线程也要访存，因此最小粒度是32。</p>
<p>在优化内存访问的时候关注的是两个特性：</p>
<ul>
<li>对齐内存访问</li>
<li>合并内存访问</li>
</ul>
<p>以下图为例，如果访存刚好是128-255，一次访存就可以</p>
<img src="/2024/10/10/CUDA%20NOTE1/4-6.png" class title="4-6">
<p>如果是127-255，就会需要两次(0-127和128-255)，用利用率来衡量的话，取得数据只有一半是用到的，利用率只有一半。更极端的场景下，利用率最低只有1/128。</p>
<p><strong>全局内存读取</strong></p>
<p>尝试从全局内存读取数据的过程为L1-&gt;L2-&gt;DRAM，编译选项<code>-Xptxa -dlcm=cg</code>会关闭L1，访存直接从L2开始，有些GPU的L1只用来存储溢出的寄存器，不用来缓存。</p>
<p>下面是一些访问缓存的对齐情况，非常直观所以不做说明。</p>
<img src="/2024/10/10/CUDA%20NOTE1/4-9.png" class title="4-9">
<img src="/2024/10/10/CUDA%20NOTE1/4-10.png" class title="4-10">
<img src="/2024/10/10/CUDA%20NOTE1/4-11.png" class title="4-11">
<img src="/2024/10/10/CUDA%20NOTE1/4-12.png" class title="4-12">
<img src="/2024/10/10/CUDA%20NOTE1/4-13.png" class title="4-13">
<p>没有L1缓存的情况是类似的，只是粒度变成了32，好处是可能提高利用率，尤其是对上述的第五种分散访存的情况。</p>
<p><strong>只读缓存</strong></p>
<p>只读缓存最初是留给纹理内存的，但在3.5以上的设备，只读缓存也可以和L1一样缓存全局内存数据，粒度为32。有两种方法指导内存从只读缓存读取。</p>
<ul>
<li>使用函数_ldg</li>
<li>在间接引用的指针上使用修饰符</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">copyKernel</span><span class="params">(<span class="type">float</span> * in,<span class="type">float</span>* out)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> idx=blockDim*blockIdx.x+threadIdx.x;</span><br><span class="line">    out[idx]=__ldg(&amp;in[idx]);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>全局内存写入</strong></p>
<p>内存的写入和读取通常在缓存中有不同的策略。对于Fermi和Kepler GPU上L1不能用于写入，写入是从L2开始的，粒度同样为32.</p>
<p><strong>结构体数组与数组结构体</strong></p>
<p>AoS是数组，元素是结构体，SoA是结构体，成员是数组。显然对于GPU而言，每个线程访问的成员是一样的，如果使用AoS，数据是不连续的。使用SoA效果会更好。</p>
<img src="/2024/10/10/CUDA%20NOTE1/4-22.png" class title="4-22">
<h3 id="3-4核函数可达到的带宽"><a href="#3-4核函数可达到的带宽" class="headerlink" title="3.4核函数可达到的带宽"></a>3.4核函数可达到的带宽</h3><p>在之前我们已经学习了两种方法改善内核性能：</p>
<ul>
<li>最大化线程束的数量隐藏内存延迟</li>
<li>对齐和合并访问</li>
</ul>
<p>如果内存访问模式本身是效率不高的，上述方式可能没有很好的效果，本节以矩阵转置为例来说明如何优化核函数的内存访问。</p>
<p>本节研究的二维矩阵的转置如下：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/4-4-1.png" alt="4-4-1" style="zoom:50%;"></p>
<p>由于在内存中数据都是一维的，所以读取时数据是连续的，写入时将是分散的：</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/4-4-2.png" alt="4-4-2" width="50%/"></p>
<p>因此如果按行读取数据，就是连续读和不连续写，而如果按列读取数据，就是不连续读和连续写。</p>
<p><img src="/2024/10/10/CUDA%20NOTE1/4-4-3.png" alt="4-4-3" style="zoom:50%;"></p>
<p>实际上，对于有限范围大小的数据，即便是非连续的读，数据也在L1中可以待后续读取使用。因此读取的利用率并没有那么低。但是反过来就不一样了，因为写入是不会写回到L1缓存，直接写回L2缓存的，因此写回是连续的效果会更好。</p>
<p>测量不同读写方式下的矩阵转置之前，需要知道GPU的理论带宽，才能知道利用了带宽的多少。这里直接使用babelStream来测量，当然不测量也是可以的，因为nsight compute提供的报告会有带宽利用率。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:UoB-HPC/BabelStream.git</span><br><span class="line">cd BabelStream</span><br><span class="line">mkdir build </span><br><span class="line">cd build</span><br><span class="line">cmake -DMODEL=cuda -DCMAKE_CUDA_COMPILER=&quot;D:\LenovoSoftstore\Install\NVDIA GPU Computing ToolkitCUDAv11.4\bin\nvcc.exe&quot; -DCUDA_ARCH=sm_75 ..</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure>
<p><img src="/2024/10/10/CUDA%20NOTE1/image-20241019201010875.png" alt="image-20241019201010875" style="zoom:50%;"></p>
<p>接下来就可以试试按行读取，不连续写入和按列读取，连续写入这两种方式的带宽：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>::chrono;</span><br><span class="line"><span class="comment">//连续读 不连续写</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">transpose1</span><span class="params">(<span class="type">double</span> *ma, <span class="type">double</span> *mb, <span class="type">int</span> nx, <span class="type">int</span> ny)</span>&#123;</span><br><span class="line">    <span class="type">int</span> ix = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> iy = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="keyword">if</span>(ix&lt;nx &amp;&amp; iy &lt;ny)&#123;</span><br><span class="line">        mb[iy + ix * ny ] = ma[ix + iy * nx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//不连续读 连续写</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">transpose2</span><span class="params">(<span class="type">double</span> *ma, <span class="type">double</span> *mb, <span class="type">int</span> nx, <span class="type">int</span> ny)</span>&#123;</span><br><span class="line">    <span class="type">int</span> ix = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> iy = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="keyword">if</span>(ix&lt;ny &amp;&amp; iy&lt;nx)&#123;</span><br><span class="line">        mb[ix + nx * iy] = ma[iy + ny * ix];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">int</span> nx = <span class="number">1</span>&lt;&lt;<span class="number">12</span>;</span><br><span class="line">    <span class="type">int</span> ny = <span class="number">1</span>&lt;&lt;<span class="number">12</span>;</span><br><span class="line">    <span class="type">double</span> *matrix_h = (<span class="type">double</span> *)<span class="built_in">std</span>::<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="type">double</span>)*nx*ny);</span><br><span class="line">    <span class="type">double</span> *matrix_da, *matrix_db;</span><br><span class="line">    cudaMalloc(&amp;matrix_da, <span class="keyword">sizeof</span>(<span class="type">double</span>)*nx*ny);</span><br><span class="line">    cudaMalloc(&amp;matrix_db, <span class="keyword">sizeof</span>(<span class="type">double</span>)*nx*ny);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> dimx = <span class="number">32</span>;</span><br><span class="line">    <span class="type">int</span> dimy = <span class="number">32</span>;</span><br><span class="line">    dim3 <span class="title function_">block</span><span class="params">(dimx, dimy)</span>;</span><br><span class="line">    dim3 <span class="title function_">grid</span><span class="params">((nx+dimx<span class="number">-1</span>)/dimx, (ny+dimy<span class="number">-1</span>)/dimy)</span>;</span><br><span class="line">    cudaMemcpy(matrix_da, matrix_h, <span class="keyword">sizeof</span>(<span class="type">double</span>)*nx*ny, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> start = high_resolution_clock::now();</span><br><span class="line">    transpose1&lt;&lt;&lt;grid, block&gt;&gt;&gt;(matrix_da, matrix_db, nx, ny);</span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line">    <span class="keyword">auto</span> end = high_resolution_clock::now();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;method 1:%lf\n&quot;</span>,duration_cast&lt;duration&lt;<span class="type">double</span>&gt;&gt;(end-start).count());</span><br><span class="line">    </span><br><span class="line">    start = high_resolution_clock::now();</span><br><span class="line">    transpose2&lt;&lt;&lt;grid, block&gt;&gt;&gt;(matrix_da, matrix_db, nx, ny);</span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line">    end = high_resolution_clock::now();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;method 2:%lf\n&quot;</span>,duration_cast&lt;duration&lt;<span class="type">double</span>&gt;&gt;(end-start).count());</span><br><span class="line"></span><br><span class="line">    cudaMemcpy(matrix_h, matrix_db, <span class="keyword">sizeof</span>(<span class="type">double</span>)*nx*ny, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>差距是相当明显的。上面代码连续运行了两个函数，为了排除缓存的影响，实际测试时每次只运行其中之一：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">method 1:0.004210</span><br><span class="line">method 2:0.003033</span><br></pre></td></tr></table></figure>
<p>连续读不连续写</p>
<img src="/2024/10/10/CUDA%20NOTE1/image-20241019210456237.png" class title="image-20241019210456237">
<p>连续写不连续读，显然写回的数据量有明显下降。内存吞吐量也有显著提升，利用的带宽明显比上一种高。</p>
<img src="/2024/10/10/CUDA%20NOTE1/image-20241019210355943.png" class title="image-20241019210355943">
<p>2.5节介绍了循环展开，对于转置这个例子也尝试进行展开，然而性能却没有什么变化，这里个人认为是因为这个核函数主要是访存受限，而循环展开在访存上并没有显著提升。我尝试运行了作者的代码，也没有性能提升，和原文记录的数据不符。</p>
<p>另外一个优化尝试是调整块的大小，发现调整块大小为8*32，在两种方式上都有显著提升，在第一种方法上性能甚至翻倍了。通过nsight comput可以看到是L2写回主存利用的带宽大大提升了，这里很可能是因为L2是共享的，并且写回的性能比读的性能更关键，减小块的大小，有利于写回时命中L2，从而实现更高的写入带宽。</p>
<p>8*32的连续读不连续写，核函数的时间为1.7msec。</p>
<img src="/2024/10/10/CUDA%20NOTE1/image-20241021102234103.png" class title="image-20241021102234103">
<p>8*32的连续写不连续读，吞吐量甚至略超过了babel stream测量的带宽，时间为1.5msec。</p>
<img src="/2024/10/10/CUDA%20NOTE1/image-20241021102549344.png" class title="image-20241021102549344">
<p>最后书上还提到了DRAM的读取如果太集中可能会排队，可以考虑让相邻的线程块映射到不相邻的数据块，避免这个问题。不过这个问题根据参考博客，是否真实存在存疑，并且上述修改块大小后，吞吐量已经略超过带宽峰值，应该不会有显著性能提升了，这里也不做测试了。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://a-y-1.github.io">橙</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://a-y-1.github.io/2024/10/10/CUDA%20NOTE1/">https://a-y-1.github.io/2024/10/10/CUDA%20NOTE1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://a-y-1.github.io" target="_blank">橙的notebook</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/">高性能计算</a></div><div class="post_share"><div class="social-share" data-image="/../cover/cuda.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/09/23/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E4%B8%8E%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" title="《并行计算与高性能计算》简记"><img class="cover" src="/../cover/parAndhpc.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">《并行计算与高性能计算》简记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/13/Arm-Performance-Lib/" title="Arm-Performance-Lib"><img class="cover" src="/../cover/ARMPL.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-13</div><div class="title">Arm-Performance-Lib</div></div></a></div><div><a href="/2023/10/13/CMU15-418notes(1-9)/" title="CMU15-418notes(1-9)"><img class="cover" src="/../cover/CMU15-418.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-13</div><div class="title">CMU15-418notes(1-9)</div></div></a></div><div><a href="/2023/10/23/CMU15-418notes(10-18)/" title="CMU15-418notes(10-18)"><img class="cover" src="/../cover/CMU15-418.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-23</div><div class="title">CMU15-418notes(10-18)</div></div></a></div><div><a href="/2023/11/19/CS149-Assignment-1&2/" title="CS149-Assignment-1&amp;2"><img class="cover" src="/../cover/hpc.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-19</div><div class="title">CS149-Assignment-1&amp;2</div></div></a></div><div><a href="/2023/11/21/CMU15-418notes(19-23)/" title="CMU15-418notes(19-23)"><img class="cover" src="/../cover/CMU15-418.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-21</div><div class="title">CMU15-418notes(19-23)</div></div></a></div><div><a href="/2023/12/21/CS149-Assignment-4/" title="CS149-Assignment-4(2023FALL)"><img class="cover" src="/../cover/hpc.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-21</div><div class="title">CS149-Assignment-4(2023FALL)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/a0.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">橙</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/A-Y-1"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CUDA-%E7%AC%94%E8%AE%B0"><span class="toc-text">CUDA 笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-CUDA-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">1 CUDA 编程模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-CUDA%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0"><span class="toc-text">1.1 CUDA并行编程模型概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">1.1.1 内存管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2-%E7%BA%BF%E7%A8%8B%E7%AE%A1%E7%90%86"><span class="toc-text">1.1.2 线程管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-3-%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-text">1.1.3 核函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-4-%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86"><span class="toc-text">1.1.4 错误处理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%A0%B8%E5%87%BD%E6%95%B0%E8%AE%A1%E6%97%B6"><span class="toc-text">1.2 核函数计时</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B"><span class="toc-text">1.3 组织并行线程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E8%AE%BE%E5%A4%87%E4%BF%A1%E6%81%AF%E6%9F%A5%E8%AF%A2"><span class="toc-text">1.4 设备信息查询</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-CUDA-%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="toc-text">2 CUDA 执行模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="toc-text">2.1 架构概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C"><span class="toc-text">2.2 线程束执行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E5%88%86%E5%8C%96"><span class="toc-text">2.2.1 线程束分化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D"><span class="toc-text">2.2.2 线程束执行的资源分配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-%E9%9A%90%E8%97%8F%E5%BB%B6%E8%BF%9F"><span class="toc-text">2.2.3 隐藏延迟</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%B9%B6%E8%A1%8C%E6%80%A7%E8%A1%A8%E7%8E%B0"><span class="toc-text">2.3 并行性表现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96"><span class="toc-text">2.4 避免分支分化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E5%BE%AA%E7%8E%AF%E5%B1%95%E5%BC%80"><span class="toc-text">2.5 循环展开</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-%E5%8A%A8%E6%80%81%E5%B9%B6%E8%A1%8C"><span class="toc-text">2.6 动态并行</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98"><span class="toc-text">3 全局内存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%A6%82%E8%BF%B0"><span class="toc-text">3.1 概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-text">3.2 内存管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1%E5%86%85%E5%AD%98%E7%9A%84%E5%88%86%E9%85%8D%EF%BC%8C%E9%87%8A%E6%94%BE%E4%B8%8E%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93"><span class="toc-text">3.2.1内存的分配，释放与数据传输</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%86%85%E5%AD%98"><span class="toc-text">3.2.2零拷贝内存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3%E7%BB%9F%E4%B8%80%E8%99%9A%E6%8B%9F%E5%AF%BB%E5%9D%80"><span class="toc-text">3.2.3统一虚拟寻址</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-4%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80"><span class="toc-text">3.2.4统一内存寻址</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.3 内存访问模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4%E6%A0%B8%E5%87%BD%E6%95%B0%E5%8F%AF%E8%BE%BE%E5%88%B0%E7%9A%84%E5%B8%A6%E5%AE%BD"><span class="toc-text">3.4核函数可达到的带宽</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/CUDA%20NOTE1/" title="CUDA笔记1">CUDA笔记1</a><time datetime="2024-10-10T04:54:29.000Z" title="发表于 2024-10-10 12:54:29">2024-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/23/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E4%B8%8E%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" title="《并行计算与高性能计算》简记">《并行计算与高性能计算》简记</a><time datetime="2024-09-23T04:54:29.000Z" title="发表于 2024-09-23 12:54:29">2024-09-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/05/Graph500/" title="Graph500">Graph500</a><time datetime="2024-08-05T14:33:29.000Z" title="发表于 2024-08-05 22:33:29">2024-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/23/MPI4py/" title="MPI4py">MPI4py</a><time datetime="2024-03-23T08:47:30.000Z" title="发表于 2024-03-23 16:47:30">2024-03-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/13/Arm-Performance-Lib/" title="Arm-Performance-Lib">Arm-Performance-Lib</a><time datetime="2024-03-13T08:15:30.000Z" title="发表于 2024-03-13 16:15:30">2024-03-13</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/../cover/cuda.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 橙</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>